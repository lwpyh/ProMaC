<html>
<head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-67FLDMD8N5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-67FLDMD8N5');
</script>

<!-- Google Tag Manager -->
<!-- <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-K8HMHBT');</script> -->
  <!-- End Google Tag Manager -->

  <!-- head -->
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="keywords" content="Prompt Mask Cycle Generation ProMac">
  <meta name="description" content="Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation. Jian Hu, Jiayi Lin, Junchi Yan, Shaogang Gong">
  <title>Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation</title>

  <script src="http://www.google.com/jsapi" type="text/javascript"></script>
  <script type="text/javascript">google.load("jquery", "1.3.2");</script>

  <link rel="stylesheet" type="text/css" href="assets/text_0804.css">

  <meta property="og:image" content="asset/TAN_teaser.png">
  <meta property="og:title" content="Temporal Alignment Networks for Long-term Video, CVPR 2022">
</head>


<body>

  <!-- Google Tag Manager (noscript) -->
<!-- <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-K8HMHBT"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> -->
  <!-- End Google Tag Manager (noscript) -->

<br>
<center>
  <span style="font-size:34px"> 
  Temporal Alignment Networks for Long-term Video </span></center>
  <br>

  <table align=center width=600px>
    <tbody><tr>
      <td align=center width=120px>
        <center>
          <span style="font-size:22px">
            <a href="https://lwpyh.github.io/" target="_blank">Jian Hu
            </a><sup>1</sup></span>
        </center>
      </td>
      <td align=center width=120px>
        <center>
          <span style="font-size:22px">
            <a href="https://jylin8100.github.io/" target="_blank">Jiayi Lin
            </a><sup>1</sup></span>
        </center>
      </td>
      <td align=center width=140px>
        <center>
          <span style="font-size:22px">
            <a href="https://thinklab.sjtu.edu.cn/" target="_blank">Junchi Yan
          </a><sup>2</sup></span>
        </center>
      </td>
      <td align=center width=140px>
        <center>
          <span style="font-size:22px">
            <a href="https://www.eecs.qmul.ac.uk/~sgg/" target="_blank">Shaogang Gong
          </a><sup>1</sup></span>
        </center>
      </td>
    </tr></tbody>
  </table>

  <table align=center width=800px>
    <tbody><tr>
    <td align=center width=800px>
      <center>
        <span style="font-size:20px">
        <sup>1</sup>Queen Mary University of London &nbsp&nbsp
        <sup>2</sup>Shanghai Jiao Tong University</span><br></br>
      </center>
    </td></tr>
    <tr>
    <td align=center width=800px>
      <center>
        <div style="font-size:20px">{htd,weidi,az}@robots.ox.ac.uk</div>
      </center>
    </td>
    </tr>
  </table>

  <br>
    <center>
      <div class="container">
        <div class="row">
          <center>
            <img src="assets/TAN_teaser.png" alt="temporal-alignment-network-figure" style="width: 100%; object-fit: cover; max-width:100%;"></a>
          </center>
          </div>
    </center>
  </center>
            
<hr>
<center><h1>Abstract</h1></center>
<div align="justify">
  <p>
The objective of this paper is a temporal alignment network 
that ingests long term video sequences, and associated text sentences, 
in order to: (1) determine if a sentence
is alignable with the video; and (2) if it is alignable, then
determine its alignment. The challenge is to train such
networks from large-scale datasets, such as HowTo100M,
where the associated text sentences have significant noise,
and are only weakly aligned when relevant.</p>
<p>Apart from proposing the alignment network, we also
make four contributions: (i) we describe a
novel co-training method that enables to denoise and train on
raw instructional videos without using manual annotation,
despite the considerable noise; (ii) to benchmark the alignment 
performance, we manually curate a 10-hour subset of
HowTo100M, totalling 80 videos, with sparse temporal descriptions. 
Our proposed model, trained on HowTo100M,
outperforms strong baselines (CLIP, MIL-NCE) on this
alignment dataset by a significant margin; (iii) we apply the
trained model in the zero-shot settings to multiple down-stream 
video understanding tasks and achieve state-of-the-art 
results, including text-video retrieval on YouCook2, and
weakly supervised video action segmentation on Breakfast-Action;
(iv) we use the automatically-aligned HowTo100M annotations for end-to-end finetuning
of the backbone model, and obtain improved performance on downstream action recognition tasks.</p>
</div>
<br>

<!-- <hr> -->
<!-- <center><h1>Video</h1></center> -->
     <!-- <center>
     <iframe width="560" height="315" src="https://www.youtube.com/embed/oMcd6maQgQk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
     </center> -->
<!-- <br> -->

<hr>
<center><h1>Publications</h1></center>

<script type="text/javascript">
function show_hide(eid) {
    var x = document.getElementById(eid);
    if (x.style.display === "none") {
      x.style.display = "block";
    } else {
      x.style.display = "none";
    }
  }
</script>

    <table align="center" width=700px>
          <tr>
                <td>
                    <a href=""><img class="layered-paper-big" style="height:175px" src="./assets/TAN_page1.png"/></a>
                </td>

                <td width=500px>
                    <span style="font-size:18px">
                    <div class="title">
                    <b>Temporal Alignment Networks for Long-term Video.</b>
                    </div>

                    <div class="authors">
                    Tengda Han,
                    Weidi Xie,
                    Andrew Zisserman
                    </div>

                    <div class="conf">
                    [Oral] CVPR, 2022
                    </div>

                    <div class="links">
                    <a href="javascript:;" onclick="show_hide('BibHan22')"> Bibtex </a> 
                   | <a href="https://www.robots.ox.ac.uk/~vgg/publications/2022/Han22a/han22a.pdf"> PDF </a> 
                   | <a href="https://arxiv.org/abs/2204.02968"> arXiv </a>
                   | <a href="https://github.com/TengdaHan/TemporalAlignNet"> Code </a> 
                   <!-- | <a href="./assets/images/CoCLR-poster-NeurIPS20.pdf"> Poster </a>  -->
                   | <a href="https://www.youtube.com/watch?v=77dcM9CyHCY"> Video </a> 
                   <!-- | <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Han20/"> All </a> -->
                    </div>

                    <div style="display: none;" class="BibtexExpand" id="BibHan22">
                    <div style="width:500px;overflow:visible;">
                    <pre class="bibtex" style="font-size:12px">
@InProceedings{Han22,
author       = "Tengda Han and Weidi Xie and Andrew Zisserman",
title        = "Temporal Alignment Networks for Long-term Video",
booktitle    = "CVPR",
year         = "2022",
}
                    </pre>
                    </div>
                    </div>

                    </span>
                </td>

          </tr>
      </table>
<br><br>

<hr>
<center><h1 id="dataset-summary">Dataset Summary
  <a href="https://www.robots.ox.ac.uk/~vgg/research/tan/#dataset-summary" style="font-size: 80%">&#182;</a>
</h1></center>
<ul>
  <li>Evaluation Dataset:
    <ul><li style="margin-top:10px;"><a href="#htm-align"><b>HTM-Align</b></a></li></ul>
  </li>
  <br>
  <li>Training Dataset:
    <table id="mytable">
      <tr>
        <td><b>Activity Category</b></td>
        <td><a href="#htm-sentencify"><b>Sentencified HTM</b></a></td>
        <td><a href="#htm-aa"><b>HTM-AA</b></a></td>
      </tr>
      <tr>
        <td>Food & Entertaining</td>
        <td>HTM-370K [&#x2714]</td>
        <td>-</td>
      </tr>
      <tr>
        <td rowspan="2">All</td>
        <td rowspan="2">HTM-1.2M [&#x2714]</td>
        <td>HTM-AA-v1 (25% subset) [&#x2714]</td>
      </tr>
      <tr>
        <td>HTM-AA-v2 (full set) [&#x2714]</td>
      </tr>
    </table>
    <center><object data="assets/pipeline.svg" width=80% style="margin-top:20px;margin-bottom:30px;"> </object></center>
  </li>
  <br>
  <li>Meta Data:
    <ul><li style="margin-top:10px;">
      <b>HowTo100M video titles</b> (77MB json)
      <a href="http://www.robots.ox.ac.uk/~htd/tan/howto100m_vid_to_title.json"><b>[Download]</b></a>
    </li></ul>
  </li>
</ul>


<hr> 
<center><h1 id="htm-align">Dataset: HTM-Align
  <a href="https://www.robots.ox.ac.uk/~vgg/research/tan/#htm-align" style="font-size: 80%">&#182;</a>
</h1></center>
<p><b>HTM-Align</b> is a <b>manually annotated</b>
80-video subset of HowTo100M (HTM) dataset, 
to evaluate the alignment performance.
It is a test set randomly sampled from Food & Entertaining category of HTM.
These videos are not used for any training.

<p>For a video from the HTM dataset, the annotators
<ul>
  <li>(1) annotate if the sentence from ASR is visually alignable with the video,</li>
  <li>(2) if alignable, change the start & end timestamps of the sentence to align with the visual content.</li>
</ul></p>

<center><p>
    <a href="http://www.robots.ox.ac.uk/~htd/tan/htm_align.json"> Download HTM-Align (616KB json) </a>
  | <a href="https://github.com/TengdaHan/TemporalAlignNet/tree/main/htm_align"> Instruction </a>
</p></center>

<center>
  <img src="assets/video_example.png" alt="anno_example" style="width: 80%; object-fit: cover; max-width:100%;"></a>
</center>

<br>

<hr> 
<center><h1 id="htm-aa">Dataset: HTM-AA 
  <a href="https://www.robots.ox.ac.uk/~vgg/research/tan/#htm-aa" style="font-size: 80%">&#182;</a>
</h1></center>

<p><b>HTM-AA</b> means Auto-Aligned (AA) version of HowTo100M (HTM) dataset.
It is an output of our Temporal Alignment Networks and a final goal of this project.
HTM-AA is a large-scale <b>paired video-text dataset</b>, automatically obtained without any human annotation.
In our paper Table 4, we show it can improve the backbone visual representation.
</p>

<p>For a video from the HowTo100M dataset, 
HTM-AA provides: 
<ul>
  <li>(1) the visually alignable sentences taken from the YouTube ASR,</li>
  <li>(2) their corresponding video timestamps (in second).</li>
</ul>
</p>
<p>

<p>We provide <u>HTM-AA version-1</u> which contains 247,564 HTM videos (about 25% subset of the entire HTM).
This subset is used in our paper Table 4 and Appendix D.1.
</p>  

<p><dev style="color:red"><b>[NEW]</b></dev> We provide <u>HTM-AA version-2</u> which contains 1,187,123 HTM videos (about 99% of the entire HTM).
Additionally, we provide the full model outputs including the alignability scores and visual-textual similarity scores of <b>all sentences</b> for future research.
</p>

<center><p>
  Download: 
  <a href="http://www.robots.ox.ac.uk/~htd/tan/htm_aa_v2.csv"> HTM-AA-v2 (2.9GB csv) </a>
  | <a href="http://www.robots.ox.ac.uk/~htd/tan/htm_aa_v2_full.tar"> HTM-AA-v2 (full outputs, 7.5GB tar) </a>
  | <a href="http://www.robots.ox.ac.uk/~htd/tan/htm_aa_v1.csv"> HTM-AA-v1 (329MB csv) </a>
</p></center>

<center><p>
<a href="htm_aa_stats.html"> Statistics </a>
  | <a href="https://github.com/TengdaHan/TemporalAlignNet/tree/main/htm_aa"> Instruction </a>
</p></center>

<br>
<center>
  <a href="htm_aa_stats.html">
  <img src="assets/wordcloud.png" alt="wordcloud" style="width: 90%; object-fit: cover; max-width:100%;"></a>
  </a>
</center>

<hr> 
<center><h1 id="htm-sentencify">Dataset: Sentencified HTM
  <a href="https://www.robots.ox.ac.uk/~vgg/research/tan/#htm-sentencify" style="font-size: 80%">&#182;</a>
</h1></center>

<p><b>Sentencified HTM</b> 
is the original <a href="https://www.di.ens.fr/willow/research/howto100m/">HowTo100M</a> (HTM) dataset, 
except the YouTube ASR (Automatic Speech Recognition) texts are converted to <b>full sentences</b>
using the method <a href="https://github.com/TengdaHan/TemporalAlignNet/tree/main/sentencify_text">here</a>.</p>
<p>For a video from the HowTo100M dataset, Sentencified HTM provides:
<ul>
  <li>(1) all of the ASR <b>sentences</b> from the YouTube,</li>
  <li>(2) the <b>original</b> start & end timestamps for each sentence (in second).</li>
</ul>
</p>

<p>We provide two versions, a subset and the fullset:
<ul>
  <li><b>HTM-370K</b>: from the "Food & Entertaining" category. 
    <center><p>
      <a href="http://www.robots.ox.ac.uk/~htd/tan/sentencified_htm_370k.json"> Download HTM-370K (3.2GB json) </a>
    | <a href="http://www.robots.ox.ac.uk/~htd/tan/htm370k_vids.txt.download"  download="htm370k_vids.txt"> Download HTM-370K video-IDs (4.5MB txt) </a>
    | <a href="htm_sentencify_stats.html"> Statistics </a>
  </p></center>
  </li>
  <li><b>HTM-1.2M</b>: from all the activity categories. 
  </li>
</ul>
<center><p>
    <a href="http://www.robots.ox.ac.uk/~htd/tan/sentencified_htm_1200k.json"> Download HTM-1.2M (9.9GB json) </a>
  | <a href="http://www.robots.ox.ac.uk/~htd/tan/htm1200k_vids.txt"> Download HTM-1.2M video-IDs (14.3MB txt) </a>
  | <a href="htm_sentencify_stats.html"> Statistics </a>
</p></center>

</p>

<br><br>
<hr>
<center><h1>Acknowledgements</h1></center>
Funding for this research is provided by EPSRC
Programme Grant VisualAI EP/T028572/1, 
a Royal Society Research Professorship RP\R1\191132 and
a Google-DeepMind Graduate Scholarship.
We thank Charig Yang, Guanqi Zhan and Chuhan Zhang for proof-reading.

<br><br>
<br><br>

<p style="text-align:center;font-size:14px;">
    Webpage template modified from <a href="https://richzhang.github.io/splitbrainauto/">Richard Zhang</a>.
</p>


</body>
</html>
